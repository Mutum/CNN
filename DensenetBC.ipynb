{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final_version_4.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "FfZUlZFw7m1I",
        "colab_type": "code",
        "outputId": "87f2b882-5555-41e6-a20c-dc4e11b42795",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 48
        }
      },
      "cell_type": "code",
      "source": [
        "# https://keras.io/\n",
        "!pip install -q keras\n",
        "import keras\n",
        "\n",
        "# 12:54 am 23OCT"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "Tio7EtC48VgJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras.datasets import cifar10\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, Input, AveragePooling2D, merge, Activation\n",
        "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
        "from keras.layers import Concatenate\n",
        "from keras.optimizers import SGD\n",
        "\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.regularizers import l2\n",
        "import keras.callbacks as callbacks\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import gridspec\n",
        "%matplotlib inline\n",
        "\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "E-x_C0Tn8WQp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# this part will prevent tensorflow to allocate all the avaliable GPU Memory\n",
        "# backend\n",
        "import tensorflow as tf\n",
        "from keras import backend as k\n",
        "\n",
        "# Don't pre-allocate memory; allocate as-needed\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "\n",
        "# Create a session with the above options specified.\n",
        "k.tensorflow_backend.set_session(tf.Session(config=config))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "y2yAdxqP8X_n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "batch_size = 64\n",
        "num_classes = 10\n",
        "epochs = 40 #170  \n",
        "l = 40\n",
        "num_filter = 32  # changes filter from 12\n",
        "compression = 0.5\n",
        "dropout_rate = 0.2 #0.2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FUqDjC0u8aB5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Load CIFAR10 Data\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "img_height, img_width, channel = x_train.shape[1],x_train.shape[2],x_train.shape[3]\n",
        "\n",
        "# convert to one hot encoing \n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nqzSMccbDOAL",
        "colab_type": "code",
        "outputId": "c0f6b8e7-737e-48ca-810b-bb0cbea66180",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 152
        }
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def grayscale(data, dtype='float32'):\n",
        "    # luma coding weighted average in video systems\n",
        "    r, g, b = np.asarray(.3, dtype=dtype), np.asarray(.59, dtype=dtype), np.asarray(.11, dtype=dtype)\n",
        "    rst = r * data[:, :, :, 0] + g * data[:, :, :, 1] + b * data[:, :, :, 2]\n",
        "    # add channel dimension\n",
        "    rst = np.expand_dims(rst, axis=3)\n",
        "    return rst\n",
        "\n",
        "X_train_gray = grayscale(x_train)\n",
        "X_test_gray = grayscale(x_test)\n",
        "\n",
        "# now we have only one channel in the images\n",
        "img_channels = 1\n",
        "\n",
        "# plot a randomly chosen image\n",
        "img = 92\n",
        "plt.figure(figsize=(4, 2))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(x_train[img], interpolation='none')\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(X_train_gray[img, :, :, 0], cmap=plt.get_cmap('gray'), interpolation='none')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAACHCAYAAADZeY9mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f9a5234aa20>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "yVVQxSiH8c17",
        "colab_type": "code",
        "outputId": "29351cfe-25ee-4b12-f7da-f65048cf6af1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Dense Block\n",
        "def add_denseblock(input, num_filter = 12, dropout_rate = 0.2):\n",
        "    global compression\n",
        "    temp = input\n",
        "    for _ in range(l):\n",
        "        BatchNorm = BatchNormalization()(temp)\n",
        "        relu = Activation('relu')(BatchNorm)\n",
        "        Conv2D_3_3 = Conv2D(int(num_filter*compression), (3,3), use_bias=False ,padding='same',kernel_initializer=\"he_normal\",kernel_regularizer=l2(1e-4))(relu) #mkernel_regularizer=l2(1e-4) changes here\n",
        "        if dropout_rate>0:\n",
        "          Conv2D_3_3 = Dropout(dropout_rate)(Conv2D_3_3)\n",
        "        concat = Concatenate(axis=-1)([temp,Conv2D_3_3])\n",
        "        \n",
        "        temp = concat\n",
        "        \n",
        "    return temp\n",
        "  \n",
        "x_train.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000, 32, 32, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "aqfhGLJJ8dcv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def add_transition(input, num_filter = 12, dropout_rate = 0.2):\n",
        "    global compression\n",
        "    BatchNorm = BatchNormalization()(input)\n",
        "    relu = Activation('relu')(BatchNorm)\n",
        "    Conv2D_BottleNeck = Conv2D(int(num_filter*compression), (1,1), use_bias=False ,padding='same',kernel_initializer=\"he_normal\")(relu) # ,kernel_regularizer=l2(1e-4\n",
        "    if dropout_rate>0:\n",
        "      Conv2D_BottleNeck = Dropout(dropout_rate)(Conv2D_BottleNeck)\n",
        "    avg = AveragePooling2D(pool_size=(2,2))(Conv2D_BottleNeck)\n",
        "    \n",
        "    return avg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OG0nuSOF8fSn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def output_layer(input):\n",
        "    global compression\n",
        "    BatchNorm = BatchNormalization()(input)\n",
        "    relu = Activation('relu')(BatchNorm)\n",
        "    AvgPooling = AveragePooling2D(pool_size=(2,2))(relu)\n",
        "    flat = Flatten()(AvgPooling)\n",
        "    output = Dense(num_classes, activation='softmax')(flat)\n",
        "    \n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rOWEZ8Ec8g-E",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "num_filter = 32\n",
        "dropout_rate = 0.2 #0.2\n",
        "l = 12\n",
        "input = Input(shape=(img_height, img_width, channel,))\n",
        "First_Conv2D = Conv2D(num_filter, (3,3), use_bias=False ,padding='same',kernel_initializer=\"he_normal\")(input) #,kernel_regularizer=l2(1e-4)\n",
        "\n",
        "First_Block = add_denseblock(First_Conv2D, num_filter, dropout_rate)\n",
        "First_Transition = add_transition(First_Block, num_filter, dropout_rate)\n",
        "\n",
        "Second_Block = add_denseblock(First_Transition, num_filter, dropout_rate)\n",
        "Second_Transition = add_transition(Second_Block, num_filter, dropout_rate)\n",
        "\n",
        "Third_Block = add_denseblock(Second_Transition, num_filter, dropout_rate)\n",
        "Third_Transition = add_transition(Third_Block, num_filter, dropout_rate)\n",
        "\n",
        "Last_Block = add_denseblock(Third_Transition,  num_filter, dropout_rate)\n",
        "output = output_layer(Last_Block)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pGk496jk8jO6",
        "colab_type": "code",
        "outputId": "25b47c1e-d835-4432-d994-d199623f59b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 9860
        }
      },
      "cell_type": "code",
      "source": [
        "model = Model(inputs=[input], outputs=[output])\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 32, 32, 32)   864         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 32, 32, 32)   128         conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 32, 32, 32)   0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 32, 32, 16)   4608        activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 32, 32, 16)   0           conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 32, 32, 48)   0           conv2d_1[0][0]                   \n",
            "                                                                 dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 32, 32, 48)   192         concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 32, 32, 48)   0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 32, 32, 16)   6912        activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 32, 32, 16)   0           conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 32, 32, 64)   0           concatenate_1[0][0]              \n",
            "                                                                 dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 32, 32, 64)   256         concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 32, 32, 64)   0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 32, 32, 16)   9216        activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 32, 32, 16)   0           conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 32, 32, 80)   0           concatenate_2[0][0]              \n",
            "                                                                 dropout_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 32, 32, 80)   320         concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 32, 32, 80)   0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 32, 32, 16)   11520       activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_4 (Dropout)             (None, 32, 32, 16)   0           conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_4 (Concatenate)     (None, 32, 32, 96)   0           concatenate_3[0][0]              \n",
            "                                                                 dropout_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 32, 32, 96)   384         concatenate_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 32, 32, 96)   0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 32, 32, 16)   13824       activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_5 (Dropout)             (None, 32, 32, 16)   0           conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_5 (Concatenate)     (None, 32, 32, 112)  0           concatenate_4[0][0]              \n",
            "                                                                 dropout_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 32, 32, 112)  448         concatenate_5[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 32, 32, 112)  0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 32, 32, 16)   16128       activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_6 (Dropout)             (None, 32, 32, 16)   0           conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_6 (Concatenate)     (None, 32, 32, 128)  0           concatenate_5[0][0]              \n",
            "                                                                 dropout_6[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 32, 32, 128)  512         concatenate_6[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 32, 32, 128)  0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 32, 32, 16)   18432       activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_7 (Dropout)             (None, 32, 32, 16)   0           conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_7 (Concatenate)     (None, 32, 32, 144)  0           concatenate_6[0][0]              \n",
            "                                                                 dropout_7[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 32, 32, 144)  576         concatenate_7[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 32, 32, 144)  0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 32, 32, 16)   20736       activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_8 (Dropout)             (None, 32, 32, 16)   0           conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_8 (Concatenate)     (None, 32, 32, 160)  0           concatenate_7[0][0]              \n",
            "                                                                 dropout_8[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 32, 32, 160)  640         concatenate_8[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 32, 32, 160)  0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 32, 32, 16)   23040       activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_9 (Dropout)             (None, 32, 32, 16)   0           conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_9 (Concatenate)     (None, 32, 32, 176)  0           concatenate_8[0][0]              \n",
            "                                                                 dropout_9[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 32, 32, 176)  704         concatenate_9[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 32, 32, 176)  0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 32, 32, 16)   25344       activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_10 (Dropout)            (None, 32, 32, 16)   0           conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_10 (Concatenate)    (None, 32, 32, 192)  0           concatenate_9[0][0]              \n",
            "                                                                 dropout_10[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 32, 32, 192)  768         concatenate_10[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 32, 32, 192)  0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 32, 32, 16)   27648       activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_11 (Dropout)            (None, 32, 32, 16)   0           conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_11 (Concatenate)    (None, 32, 32, 208)  0           concatenate_10[0][0]             \n",
            "                                                                 dropout_11[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 32, 32, 208)  832         concatenate_11[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 32, 32, 208)  0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 32, 32, 16)   29952       activation_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_12 (Dropout)            (None, 32, 32, 16)   0           conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_12 (Concatenate)    (None, 32, 32, 224)  0           concatenate_11[0][0]             \n",
            "                                                                 dropout_12[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 32, 32, 224)  896         concatenate_12[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 32, 32, 224)  0           batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 32, 32, 16)   3584        activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_13 (Dropout)            (None, 32, 32, 16)   0           conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_1 (AveragePoo (None, 16, 16, 16)   0           dropout_13[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 16, 16, 16)   64          average_pooling2d_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 16, 16, 16)   0           batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 16, 16, 16)   2304        activation_14[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_14 (Dropout)            (None, 16, 16, 16)   0           conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_13 (Concatenate)    (None, 16, 16, 32)   0           average_pooling2d_1[0][0]        \n",
            "                                                                 dropout_14[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 16, 16, 32)   128         concatenate_13[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 16, 16, 32)   0           batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 16, 16, 16)   4608        activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_15 (Dropout)            (None, 16, 16, 16)   0           conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_14 (Concatenate)    (None, 16, 16, 48)   0           concatenate_13[0][0]             \n",
            "                                                                 dropout_15[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 16, 16, 48)   192         concatenate_14[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 16, 16, 48)   0           batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 16, 16, 16)   6912        activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_16 (Dropout)            (None, 16, 16, 16)   0           conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_15 (Concatenate)    (None, 16, 16, 64)   0           concatenate_14[0][0]             \n",
            "                                                                 dropout_16[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_17 (BatchNo (None, 16, 16, 64)   256         concatenate_15[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 16, 16, 64)   0           batch_normalization_17[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 16, 16, 16)   9216        activation_17[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_17 (Dropout)            (None, 16, 16, 16)   0           conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_16 (Concatenate)    (None, 16, 16, 80)   0           concatenate_15[0][0]             \n",
            "                                                                 dropout_17[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_18 (BatchNo (None, 16, 16, 80)   320         concatenate_16[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, 16, 16, 80)   0           batch_normalization_18[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 16, 16, 16)   11520       activation_18[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_18 (Dropout)            (None, 16, 16, 16)   0           conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_17 (Concatenate)    (None, 16, 16, 96)   0           concatenate_16[0][0]             \n",
            "                                                                 dropout_18[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_19 (BatchNo (None, 16, 16, 96)   384         concatenate_17[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_19 (Activation)      (None, 16, 16, 96)   0           batch_normalization_19[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 16, 16, 16)   13824       activation_19[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_19 (Dropout)            (None, 16, 16, 16)   0           conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_18 (Concatenate)    (None, 16, 16, 112)  0           concatenate_17[0][0]             \n",
            "                                                                 dropout_19[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_20 (BatchNo (None, 16, 16, 112)  448         concatenate_18[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_20 (Activation)      (None, 16, 16, 112)  0           batch_normalization_20[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 16, 16, 16)   16128       activation_20[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_20 (Dropout)            (None, 16, 16, 16)   0           conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_19 (Concatenate)    (None, 16, 16, 128)  0           concatenate_18[0][0]             \n",
            "                                                                 dropout_20[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_21 (BatchNo (None, 16, 16, 128)  512         concatenate_19[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_21 (Activation)      (None, 16, 16, 128)  0           batch_normalization_21[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_22 (Conv2D)              (None, 16, 16, 16)   18432       activation_21[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_21 (Dropout)            (None, 16, 16, 16)   0           conv2d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_20 (Concatenate)    (None, 16, 16, 144)  0           concatenate_19[0][0]             \n",
            "                                                                 dropout_21[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_22 (BatchNo (None, 16, 16, 144)  576         concatenate_20[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_22 (Activation)      (None, 16, 16, 144)  0           batch_normalization_22[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_23 (Conv2D)              (None, 16, 16, 16)   20736       activation_22[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_22 (Dropout)            (None, 16, 16, 16)   0           conv2d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_21 (Concatenate)    (None, 16, 16, 160)  0           concatenate_20[0][0]             \n",
            "                                                                 dropout_22[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_23 (BatchNo (None, 16, 16, 160)  640         concatenate_21[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_23 (Activation)      (None, 16, 16, 160)  0           batch_normalization_23[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_24 (Conv2D)              (None, 16, 16, 16)   23040       activation_23[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_23 (Dropout)            (None, 16, 16, 16)   0           conv2d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_22 (Concatenate)    (None, 16, 16, 176)  0           concatenate_21[0][0]             \n",
            "                                                                 dropout_23[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_24 (BatchNo (None, 16, 16, 176)  704         concatenate_22[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_24 (Activation)      (None, 16, 16, 176)  0           batch_normalization_24[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_25 (Conv2D)              (None, 16, 16, 16)   25344       activation_24[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_24 (Dropout)            (None, 16, 16, 16)   0           conv2d_25[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_23 (Concatenate)    (None, 16, 16, 192)  0           concatenate_22[0][0]             \n",
            "                                                                 dropout_24[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_25 (BatchNo (None, 16, 16, 192)  768         concatenate_23[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_25 (Activation)      (None, 16, 16, 192)  0           batch_normalization_25[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_26 (Conv2D)              (None, 16, 16, 16)   27648       activation_25[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_25 (Dropout)            (None, 16, 16, 16)   0           conv2d_26[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_24 (Concatenate)    (None, 16, 16, 208)  0           concatenate_23[0][0]             \n",
            "                                                                 dropout_25[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_26 (BatchNo (None, 16, 16, 208)  832         concatenate_24[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_26 (Activation)      (None, 16, 16, 208)  0           batch_normalization_26[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_27 (Conv2D)              (None, 16, 16, 16)   3328        activation_26[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_26 (Dropout)            (None, 16, 16, 16)   0           conv2d_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_2 (AveragePoo (None, 8, 8, 16)     0           dropout_26[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_27 (BatchNo (None, 8, 8, 16)     64          average_pooling2d_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_27 (Activation)      (None, 8, 8, 16)     0           batch_normalization_27[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_28 (Conv2D)              (None, 8, 8, 16)     2304        activation_27[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_27 (Dropout)            (None, 8, 8, 16)     0           conv2d_28[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_25 (Concatenate)    (None, 8, 8, 32)     0           average_pooling2d_2[0][0]        \n",
            "                                                                 dropout_27[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_28 (BatchNo (None, 8, 8, 32)     128         concatenate_25[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_28 (Activation)      (None, 8, 8, 32)     0           batch_normalization_28[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_29 (Conv2D)              (None, 8, 8, 16)     4608        activation_28[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_28 (Dropout)            (None, 8, 8, 16)     0           conv2d_29[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_26 (Concatenate)    (None, 8, 8, 48)     0           concatenate_25[0][0]             \n",
            "                                                                 dropout_28[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_29 (BatchNo (None, 8, 8, 48)     192         concatenate_26[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_29 (Activation)      (None, 8, 8, 48)     0           batch_normalization_29[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_30 (Conv2D)              (None, 8, 8, 16)     6912        activation_29[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_29 (Dropout)            (None, 8, 8, 16)     0           conv2d_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_27 (Concatenate)    (None, 8, 8, 64)     0           concatenate_26[0][0]             \n",
            "                                                                 dropout_29[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_30 (BatchNo (None, 8, 8, 64)     256         concatenate_27[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_30 (Activation)      (None, 8, 8, 64)     0           batch_normalization_30[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_31 (Conv2D)              (None, 8, 8, 16)     9216        activation_30[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_30 (Dropout)            (None, 8, 8, 16)     0           conv2d_31[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_28 (Concatenate)    (None, 8, 8, 80)     0           concatenate_27[0][0]             \n",
            "                                                                 dropout_30[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_31 (BatchNo (None, 8, 8, 80)     320         concatenate_28[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_31 (Activation)      (None, 8, 8, 80)     0           batch_normalization_31[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_32 (Conv2D)              (None, 8, 8, 16)     11520       activation_31[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_31 (Dropout)            (None, 8, 8, 16)     0           conv2d_32[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_29 (Concatenate)    (None, 8, 8, 96)     0           concatenate_28[0][0]             \n",
            "                                                                 dropout_31[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_32 (BatchNo (None, 8, 8, 96)     384         concatenate_29[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_32 (Activation)      (None, 8, 8, 96)     0           batch_normalization_32[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_33 (Conv2D)              (None, 8, 8, 16)     13824       activation_32[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_32 (Dropout)            (None, 8, 8, 16)     0           conv2d_33[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_30 (Concatenate)    (None, 8, 8, 112)    0           concatenate_29[0][0]             \n",
            "                                                                 dropout_32[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_33 (BatchNo (None, 8, 8, 112)    448         concatenate_30[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_33 (Activation)      (None, 8, 8, 112)    0           batch_normalization_33[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_34 (Conv2D)              (None, 8, 8, 16)     16128       activation_33[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_33 (Dropout)            (None, 8, 8, 16)     0           conv2d_34[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_31 (Concatenate)    (None, 8, 8, 128)    0           concatenate_30[0][0]             \n",
            "                                                                 dropout_33[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_34 (BatchNo (None, 8, 8, 128)    512         concatenate_31[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_34 (Activation)      (None, 8, 8, 128)    0           batch_normalization_34[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_35 (Conv2D)              (None, 8, 8, 16)     18432       activation_34[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_34 (Dropout)            (None, 8, 8, 16)     0           conv2d_35[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_32 (Concatenate)    (None, 8, 8, 144)    0           concatenate_31[0][0]             \n",
            "                                                                 dropout_34[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_35 (BatchNo (None, 8, 8, 144)    576         concatenate_32[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_35 (Activation)      (None, 8, 8, 144)    0           batch_normalization_35[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_36 (Conv2D)              (None, 8, 8, 16)     20736       activation_35[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_35 (Dropout)            (None, 8, 8, 16)     0           conv2d_36[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_33 (Concatenate)    (None, 8, 8, 160)    0           concatenate_32[0][0]             \n",
            "                                                                 dropout_35[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_36 (BatchNo (None, 8, 8, 160)    640         concatenate_33[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_36 (Activation)      (None, 8, 8, 160)    0           batch_normalization_36[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_37 (Conv2D)              (None, 8, 8, 16)     23040       activation_36[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_36 (Dropout)            (None, 8, 8, 16)     0           conv2d_37[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_34 (Concatenate)    (None, 8, 8, 176)    0           concatenate_33[0][0]             \n",
            "                                                                 dropout_36[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_37 (BatchNo (None, 8, 8, 176)    704         concatenate_34[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_37 (Activation)      (None, 8, 8, 176)    0           batch_normalization_37[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_38 (Conv2D)              (None, 8, 8, 16)     25344       activation_37[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_37 (Dropout)            (None, 8, 8, 16)     0           conv2d_38[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_35 (Concatenate)    (None, 8, 8, 192)    0           concatenate_34[0][0]             \n",
            "                                                                 dropout_37[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_38 (BatchNo (None, 8, 8, 192)    768         concatenate_35[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_38 (Activation)      (None, 8, 8, 192)    0           batch_normalization_38[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_39 (Conv2D)              (None, 8, 8, 16)     27648       activation_38[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_38 (Dropout)            (None, 8, 8, 16)     0           conv2d_39[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_36 (Concatenate)    (None, 8, 8, 208)    0           concatenate_35[0][0]             \n",
            "                                                                 dropout_38[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_39 (BatchNo (None, 8, 8, 208)    832         concatenate_36[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_39 (Activation)      (None, 8, 8, 208)    0           batch_normalization_39[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_40 (Conv2D)              (None, 8, 8, 16)     3328        activation_39[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_39 (Dropout)            (None, 8, 8, 16)     0           conv2d_40[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_3 (AveragePoo (None, 4, 4, 16)     0           dropout_39[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_40 (BatchNo (None, 4, 4, 16)     64          average_pooling2d_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_40 (Activation)      (None, 4, 4, 16)     0           batch_normalization_40[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_41 (Conv2D)              (None, 4, 4, 16)     2304        activation_40[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_40 (Dropout)            (None, 4, 4, 16)     0           conv2d_41[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_37 (Concatenate)    (None, 4, 4, 32)     0           average_pooling2d_3[0][0]        \n",
            "                                                                 dropout_40[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_41 (BatchNo (None, 4, 4, 32)     128         concatenate_37[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_41 (Activation)      (None, 4, 4, 32)     0           batch_normalization_41[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_42 (Conv2D)              (None, 4, 4, 16)     4608        activation_41[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_41 (Dropout)            (None, 4, 4, 16)     0           conv2d_42[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_38 (Concatenate)    (None, 4, 4, 48)     0           concatenate_37[0][0]             \n",
            "                                                                 dropout_41[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_42 (BatchNo (None, 4, 4, 48)     192         concatenate_38[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_42 (Activation)      (None, 4, 4, 48)     0           batch_normalization_42[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_43 (Conv2D)              (None, 4, 4, 16)     6912        activation_42[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_42 (Dropout)            (None, 4, 4, 16)     0           conv2d_43[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_39 (Concatenate)    (None, 4, 4, 64)     0           concatenate_38[0][0]             \n",
            "                                                                 dropout_42[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_43 (BatchNo (None, 4, 4, 64)     256         concatenate_39[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_43 (Activation)      (None, 4, 4, 64)     0           batch_normalization_43[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_44 (Conv2D)              (None, 4, 4, 16)     9216        activation_43[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_43 (Dropout)            (None, 4, 4, 16)     0           conv2d_44[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_40 (Concatenate)    (None, 4, 4, 80)     0           concatenate_39[0][0]             \n",
            "                                                                 dropout_43[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_44 (BatchNo (None, 4, 4, 80)     320         concatenate_40[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_44 (Activation)      (None, 4, 4, 80)     0           batch_normalization_44[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_45 (Conv2D)              (None, 4, 4, 16)     11520       activation_44[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_44 (Dropout)            (None, 4, 4, 16)     0           conv2d_45[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_41 (Concatenate)    (None, 4, 4, 96)     0           concatenate_40[0][0]             \n",
            "                                                                 dropout_44[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_45 (BatchNo (None, 4, 4, 96)     384         concatenate_41[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_45 (Activation)      (None, 4, 4, 96)     0           batch_normalization_45[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_46 (Conv2D)              (None, 4, 4, 16)     13824       activation_45[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_45 (Dropout)            (None, 4, 4, 16)     0           conv2d_46[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_42 (Concatenate)    (None, 4, 4, 112)    0           concatenate_41[0][0]             \n",
            "                                                                 dropout_45[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_46 (BatchNo (None, 4, 4, 112)    448         concatenate_42[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_46 (Activation)      (None, 4, 4, 112)    0           batch_normalization_46[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_47 (Conv2D)              (None, 4, 4, 16)     16128       activation_46[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_46 (Dropout)            (None, 4, 4, 16)     0           conv2d_47[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_43 (Concatenate)    (None, 4, 4, 128)    0           concatenate_42[0][0]             \n",
            "                                                                 dropout_46[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_47 (BatchNo (None, 4, 4, 128)    512         concatenate_43[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_47 (Activation)      (None, 4, 4, 128)    0           batch_normalization_47[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_48 (Conv2D)              (None, 4, 4, 16)     18432       activation_47[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_47 (Dropout)            (None, 4, 4, 16)     0           conv2d_48[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_44 (Concatenate)    (None, 4, 4, 144)    0           concatenate_43[0][0]             \n",
            "                                                                 dropout_47[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_48 (BatchNo (None, 4, 4, 144)    576         concatenate_44[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_48 (Activation)      (None, 4, 4, 144)    0           batch_normalization_48[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_49 (Conv2D)              (None, 4, 4, 16)     20736       activation_48[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_48 (Dropout)            (None, 4, 4, 16)     0           conv2d_49[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_45 (Concatenate)    (None, 4, 4, 160)    0           concatenate_44[0][0]             \n",
            "                                                                 dropout_48[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_49 (BatchNo (None, 4, 4, 160)    640         concatenate_45[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_49 (Activation)      (None, 4, 4, 160)    0           batch_normalization_49[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_50 (Conv2D)              (None, 4, 4, 16)     23040       activation_49[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_49 (Dropout)            (None, 4, 4, 16)     0           conv2d_50[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_46 (Concatenate)    (None, 4, 4, 176)    0           concatenate_45[0][0]             \n",
            "                                                                 dropout_49[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_50 (BatchNo (None, 4, 4, 176)    704         concatenate_46[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_50 (Activation)      (None, 4, 4, 176)    0           batch_normalization_50[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_51 (Conv2D)              (None, 4, 4, 16)     25344       activation_50[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_50 (Dropout)            (None, 4, 4, 16)     0           conv2d_51[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_47 (Concatenate)    (None, 4, 4, 192)    0           concatenate_46[0][0]             \n",
            "                                                                 dropout_50[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_51 (BatchNo (None, 4, 4, 192)    768         concatenate_47[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_51 (Activation)      (None, 4, 4, 192)    0           batch_normalization_51[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_52 (Conv2D)              (None, 4, 4, 16)     27648       activation_51[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_51 (Dropout)            (None, 4, 4, 16)     0           conv2d_52[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_48 (Concatenate)    (None, 4, 4, 208)    0           concatenate_47[0][0]             \n",
            "                                                                 dropout_51[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_52 (BatchNo (None, 4, 4, 208)    832         concatenate_48[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_52 (Activation)      (None, 4, 4, 208)    0           batch_normalization_52[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_4 (AveragePoo (None, 2, 2, 208)    0           activation_52[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 832)          0           average_pooling2d_4[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 10)           8330        flatten_1[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 790,058\n",
            "Trainable params: 777,994\n",
            "Non-trainable params: 12,064\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dxIZmnfm8oG7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# def lr_schedule(epochs):\n",
        "#     lrate = 0.1\n",
        "#     if epochs >17:\n",
        "#       lrate = 0.01\n",
        "#     elif epochs >= 30:\n",
        "#       lrate=0.001\n",
        "#     elif epochs >= 60:\n",
        "#       lrate=0.0001\n",
        "#     elif epochs >=80:\n",
        "#       lrate=0.00001\n",
        "#     elif epochs >=100:\n",
        "#       lrate=0.000001\n",
        "#     elif epochs >=110:\n",
        "#       lrate=0.0001\n",
        "#     elif epochs > 125:\n",
        "#       lrate=0.0000001\n",
        "# #     elif epochs > 100:\n",
        "# #       lrate=0.01\n",
        "      \n",
        "#     return lrate\n",
        "\n",
        "\n",
        "def lr_schedule(epochs):\n",
        "    lrate = 0.1\n",
        "    if epochs >19:\n",
        "      lrate = 0.01\n",
        "    elif epochs >= 30:\n",
        "      lrate=0.001     \n",
        "    return lrate"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iDrfx5b28rz7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sgd_ = SGD(lr=0.1, momentum=0.9,decay=1e-6, nesterov=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Rnvf7yuu8sin",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# determine Loss function and Optimizer\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=sgd_,\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yJeo_bx58ubQ",
        "colab_type": "code",
        "outputId": "f17d03ae-a06f-422f-860a-10e2a4e4c620",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 65
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials \n",
        "\n",
        "\n",
        "# 1. Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "oN-_56rr82Rr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "filepath=\"/content/gdrive/My Drive/Final_version_1_weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max',save_weights_only=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eJkePUyT84qi",
        "colab_type": "code",
        "outputId": "919f38e9-8505-4282-fe81-ee84db747b53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        }
      },
      "cell_type": "code",
      "source": [
        "datagen = ImageDataGenerator(\n",
        "    featurewise_center=False,\n",
        "    samplewise_center=False,\n",
        "    featurewise_std_normalization=False,\n",
        "    samplewise_std_normalization=False,\n",
        "    zca_whitening=False,\n",
        "    rotation_range=10,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=False\n",
        "    )\n",
        "datagen.fit(x_train)\n",
        "\n",
        "# visualize augmented points\n",
        "plt.figure(figsize=(6, 6))\n",
        "(X_batch, Y_batch) = datagen.flow(X_train_gray, y_train, batch_size=9).next()\n",
        "for i in range(9):\n",
        "    plt.subplot(3, 3, (i + 1))\n",
        "    plt.imshow(X_batch[i, :, :, 0], cmap=plt.get_cmap('gray'), interpolation='none')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWwAAAFoCAYAAABzDsGvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f9a8c8f6978>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "ZQXBZJpzCHdA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# (x_train.shape[0] // (batch_size/2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2bOKL9Pb86wN",
        "colab_type": "code",
        "outputId": "183c2d25-174b-41ee-c206-625b9188e50a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2604
        }
      },
      "cell_type": "code",
      "source": [
        "model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
        "                    steps_per_epoch=(x_train.shape[0] // (batch_size/2)), epochs=epochs, verbose=1,\n",
        "                    validation_data=(x_test,y_test),\n",
        "                 callbacks=[LearningRateScheduler(lr_schedule),checkpoint])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Variable *= will be deprecated. Use `var.assign(var * other)` if you want assignment to the variable value or `x = x * y` if you want a new python Tensor object.\n",
            "Epoch 1/40\n",
            "1562/1562 [==============================] - 529s 339ms/step - loss: 2.2041 - acc: 0.3020 - val_loss: 1.9606 - val_acc: 0.3606\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.36060, saving model to /content/gdrive/My Drive/Final_version_1_weights-improvement-01-0.36.hdf5\n",
            "Epoch 2/40\n",
            "1562/1562 [==============================] - 508s 325ms/step - loss: 1.7110 - acc: 0.4232 - val_loss: 1.9539 - val_acc: 0.4044\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.36060 to 0.40440, saving model to /content/gdrive/My Drive/Final_version_1_weights-improvement-02-0.40.hdf5\n",
            "Epoch 3/40\n",
            "1562/1562 [==============================] - 509s 326ms/step - loss: 1.4193 - acc: 0.5258 - val_loss: 1.7316 - val_acc: 0.4852\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.40440 to 0.48520, saving model to /content/gdrive/My Drive/Final_version_1_weights-improvement-03-0.49.hdf5\n",
            "Epoch 4/40\n",
            "1562/1562 [==============================] - 511s 327ms/step - loss: 1.2184 - acc: 0.6015 - val_loss: 1.7928 - val_acc: 0.5254\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.48520 to 0.52540, saving model to /content/gdrive/My Drive/Final_version_1_weights-improvement-04-0.53.hdf5\n",
            "Epoch 5/40\n",
            "1562/1562 [==============================] - 506s 324ms/step - loss: 1.0883 - acc: 0.6534 - val_loss: 1.6871 - val_acc: 0.5691\n",
            "\n",
            "Epoch 00005: val_acc improved from 0.52540 to 0.56910, saving model to /content/gdrive/My Drive/Final_version_1_weights-improvement-05-0.57.hdf5\n",
            "Epoch 6/40\n",
            "1562/1562 [==============================] - 510s 326ms/step - loss: 0.9843 - acc: 0.6966 - val_loss: 1.1844 - val_acc: 0.6792\n",
            "\n",
            "Epoch 00006: val_acc improved from 0.56910 to 0.67920, saving model to /content/gdrive/My Drive/Final_version_1_weights-improvement-06-0.68.hdf5\n",
            "Epoch 7/40\n",
            "1562/1562 [==============================] - 513s 329ms/step - loss: 0.9159 - acc: 0.7270 - val_loss: 1.2007 - val_acc: 0.6743\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.67920\n",
            "Epoch 8/40\n",
            "1562/1562 [==============================] - 508s 325ms/step - loss: 0.8712 - acc: 0.7489 - val_loss: 1.2210 - val_acc: 0.6908\n",
            "\n",
            "Epoch 00008: val_acc improved from 0.67920 to 0.69080, saving model to /content/gdrive/My Drive/Final_version_1_weights-improvement-08-0.69.hdf5\n",
            "Epoch 9/40\n",
            "1562/1562 [==============================] - 503s 322ms/step - loss: 0.8398 - acc: 0.7627 - val_loss: 0.9339 - val_acc: 0.7365\n",
            "\n",
            "Epoch 00009: val_acc improved from 0.69080 to 0.73650, saving model to /content/gdrive/My Drive/Final_version_1_weights-improvement-09-0.74.hdf5\n",
            "Epoch 10/40\n",
            "1562/1562 [==============================] - 498s 319ms/step - loss: 0.8101 - acc: 0.7777 - val_loss: 1.0328 - val_acc: 0.7419\n",
            "\n",
            "Epoch 00010: val_acc improved from 0.73650 to 0.74190, saving model to /content/gdrive/My Drive/Final_version_1_weights-improvement-10-0.74.hdf5\n",
            "Epoch 11/40\n",
            "1562/1562 [==============================] - 508s 325ms/step - loss: 0.7959 - acc: 0.7857 - val_loss: 0.9495 - val_acc: 0.7529\n",
            "\n",
            "Epoch 00011: val_acc improved from 0.74190 to 0.75290, saving model to /content/gdrive/My Drive/Final_version_1_weights-improvement-11-0.75.hdf5\n",
            "Epoch 12/40\n",
            "1562/1562 [==============================] - 511s 327ms/step - loss: 0.7782 - acc: 0.7934 - val_loss: 1.1311 - val_acc: 0.7266\n",
            "\n",
            "Epoch 00012: val_acc did not improve from 0.75290\n",
            "Epoch 13/40\n",
            "1562/1562 [==============================] - 511s 327ms/step - loss: 0.7655 - acc: 0.7993 - val_loss: 0.7981 - val_acc: 0.8036\n",
            "\n",
            "Epoch 00013: val_acc improved from 0.75290 to 0.80360, saving model to /content/gdrive/My Drive/Final_version_1_weights-improvement-13-0.80.hdf5\n",
            "Epoch 14/40\n",
            "1562/1562 [==============================] - 509s 326ms/step - loss: 0.7560 - acc: 0.8055 - val_loss: 0.8148 - val_acc: 0.7957\n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.80360\n",
            "Epoch 15/40\n",
            "1562/1562 [==============================] - 512s 328ms/step - loss: 0.7416 - acc: 0.8110 - val_loss: 0.8195 - val_acc: 0.7950\n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.80360\n",
            "Epoch 16/40\n",
            "1562/1562 [==============================] - 507s 324ms/step - loss: 0.7297 - acc: 0.8168 - val_loss: 1.1943 - val_acc: 0.7080\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.80360\n",
            "Epoch 17/40\n",
            "1562/1562 [==============================] - 510s 326ms/step - loss: 0.7248 - acc: 0.8181 - val_loss: 1.0026 - val_acc: 0.7562\n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.80360\n",
            "Epoch 18/40\n",
            "1562/1562 [==============================] - 515s 330ms/step - loss: 0.7231 - acc: 0.8205 - val_loss: 0.8639 - val_acc: 0.7889\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.80360\n",
            "Epoch 19/40\n",
            "1562/1562 [==============================] - 516s 330ms/step - loss: 0.7166 - acc: 0.8239 - val_loss: 1.0928 - val_acc: 0.7498\n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.80360\n",
            "Epoch 20/40\n",
            "1562/1562 [==============================] - 509s 326ms/step - loss: 0.7086 - acc: 0.8269 - val_loss: 1.1127 - val_acc: 0.7398\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.80360\n",
            "Epoch 21/40\n",
            "1562/1562 [==============================] - 514s 329ms/step - loss: 0.5820 - acc: 0.8686 - val_loss: 0.5647 - val_acc: 0.8836\n",
            "\n",
            "Epoch 00021: val_acc improved from 0.80360 to 0.88360, saving model to /content/gdrive/My Drive/Final_version_1_weights-improvement-21-0.88.hdf5\n",
            "Epoch 22/40\n",
            "1562/1562 [==============================] - 513s 328ms/step - loss: 0.5323 - acc: 0.8822 - val_loss: 0.5540 - val_acc: 0.8848\n",
            "\n",
            "Epoch 00022: val_acc improved from 0.88360 to 0.88480, saving model to /content/gdrive/My Drive/Final_version_1_weights-improvement-22-0.88.hdf5\n",
            "Epoch 23/40\n",
            "1562/1562 [==============================] - 512s 328ms/step - loss: 0.5082 - acc: 0.8869 - val_loss: 0.5349 - val_acc: 0.8873\n",
            "\n",
            "Epoch 00023: val_acc improved from 0.88480 to 0.88730, saving model to /content/gdrive/My Drive/Final_version_1_weights-improvement-23-0.89.hdf5\n",
            "Epoch 24/40\n",
            "1562/1562 [==============================] - 512s 328ms/step - loss: 0.4884 - acc: 0.8916 - val_loss: 0.5579 - val_acc: 0.8772\n",
            "\n",
            "Epoch 00024: val_acc did not improve from 0.88730\n",
            "Epoch 25/40\n",
            "1562/1562 [==============================] - 507s 325ms/step - loss: 0.4697 - acc: 0.8944 - val_loss: 0.5515 - val_acc: 0.8803\n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.88730\n",
            "Epoch 26/40\n",
            "1562/1562 [==============================] - 516s 330ms/step - loss: 0.4551 - acc: 0.8968 - val_loss: 0.5152 - val_acc: 0.8871\n",
            "\n",
            "Epoch 00026: val_acc did not improve from 0.88730\n",
            "Epoch 27/40\n",
            "1562/1562 [==============================] - 510s 327ms/step - loss: 0.4427 - acc: 0.8977 - val_loss: 0.4842 - val_acc: 0.8907\n",
            "\n",
            "Epoch 00027: val_acc improved from 0.88730 to 0.89070, saving model to /content/gdrive/My Drive/Final_version_1_weights-improvement-27-0.89.hdf5\n",
            "Epoch 28/40\n",
            "1562/1562 [==============================] - 509s 326ms/step - loss: 0.4311 - acc: 0.9005 - val_loss: 0.5324 - val_acc: 0.8749\n",
            "\n",
            "Epoch 00028: val_acc did not improve from 0.89070\n",
            "Epoch 29/40\n",
            "1562/1562 [==============================] - 513s 328ms/step - loss: 0.4200 - acc: 0.9004 - val_loss: 0.5224 - val_acc: 0.8841\n",
            "\n",
            "Epoch 00029: val_acc did not improve from 0.89070\n",
            "Epoch 30/40\n",
            "1562/1562 [==============================] - 508s 325ms/step - loss: 0.4131 - acc: 0.9028 - val_loss: 0.4906 - val_acc: 0.8898\n",
            "\n",
            "Epoch 00030: val_acc did not improve from 0.89070\n",
            "Epoch 31/40\n",
            "1562/1562 [==============================] - 513s 329ms/step - loss: 0.4049 - acc: 0.9037 - val_loss: 0.5136 - val_acc: 0.8817\n",
            "\n",
            "Epoch 00031: val_acc did not improve from 0.89070\n",
            "Epoch 32/40\n",
            "1562/1562 [==============================] - 511s 327ms/step - loss: 0.4007 - acc: 0.9032 - val_loss: 0.5077 - val_acc: 0.8803\n",
            "\n",
            "Epoch 00032: val_acc did not improve from 0.89070\n",
            "Epoch 33/40\n",
            "1562/1562 [==============================] - 508s 325ms/step - loss: 0.3916 - acc: 0.9058 - val_loss: 0.4581 - val_acc: 0.8911\n",
            "\n",
            "Epoch 00033: val_acc improved from 0.89070 to 0.89110, saving model to /content/gdrive/My Drive/Final_version_1_weights-improvement-33-0.89.hdf5\n",
            "Epoch 34/40\n",
            "1562/1562 [==============================] - 514s 329ms/step - loss: 0.3859 - acc: 0.9061 - val_loss: 0.4528 - val_acc: 0.8948\n",
            "\n",
            "Epoch 00034: val_acc improved from 0.89110 to 0.89480, saving model to /content/gdrive/My Drive/Final_version_1_weights-improvement-34-0.89.hdf5\n",
            "Epoch 35/40\n",
            "1562/1562 [==============================] - 499s 320ms/step - loss: 0.3799 - acc: 0.9080 - val_loss: 0.5596 - val_acc: 0.8700\n",
            "\n",
            "Epoch 00035: val_acc did not improve from 0.89480\n",
            "Epoch 36/40\n",
            "1562/1562 [==============================] - 508s 325ms/step - loss: 0.3759 - acc: 0.9073 - val_loss: 0.4985 - val_acc: 0.8822\n",
            "\n",
            "Epoch 00036: val_acc did not improve from 0.89480\n",
            "Epoch 37/40\n",
            "1562/1562 [==============================] - 509s 326ms/step - loss: 0.3753 - acc: 0.9070 - val_loss: 0.4978 - val_acc: 0.8815\n",
            "\n",
            "Epoch 00037: val_acc did not improve from 0.89480\n",
            "Epoch 38/40\n",
            "1155/1562 [=====================>........] - ETA: 2:10 - loss: 0.3740 - acc: 0.9077Buffered data was truncated after reaching the output size limit."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Ohiabmy7p9vu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def lr_schedule(epochs):\n",
        "    lrate = 0.001\n",
        "    if epochs >19:\n",
        "      lrate = 0.0001\n",
        "    elif epochs >= 25:\n",
        "      lrate=0.001     \n",
        "    return lrate\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D9cybNC6_8U_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.load_weights('/content/gdrive/My Drive/Final_version_1_weights-improvement-34-0.89.hdf5')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hai9noNapdb8",
        "colab_type": "code",
        "outputId": "6a3224d2-9412-41e9-a3ab-f341cee3b9f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2791
        }
      },
      "cell_type": "code",
      "source": [
        "model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
        "                    steps_per_epoch=(x_train.shape[0] // (batch_size/2)), epochs=epochs, verbose=1,\n",
        "                    validation_data=(x_test,y_test),\n",
        "                 callbacks=[LearningRateScheduler(lr_schedule),checkpoint])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Variable *= will be deprecated. Use `var.assign(var * other)` if you want assignment to the variable value or `x = x * y` if you want a new python Tensor object.\n",
            "Epoch 1/40\n",
            "1562/1562 [==============================] - 520s 333ms/step - loss: 0.3551 - acc: 0.9166 - val_loss: 0.4265 - val_acc: 0.9052\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.90520, saving model to /content/gdrive/My Drive/Final_version_1_weights-improvement-01-0.91.hdf5\n",
            "Epoch 2/40\n",
            "1562/1562 [==============================] - 503s 322ms/step - loss: 0.3427 - acc: 0.9205 - val_loss: 0.4187 - val_acc: 0.9082\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.90520 to 0.90820, saving model to /content/gdrive/My Drive/Final_version_1_weights-improvement-02-0.91.hdf5\n",
            "Epoch 3/40\n",
            "1562/1562 [==============================] - 498s 319ms/step - loss: 0.3383 - acc: 0.9214 - val_loss: 0.4262 - val_acc: 0.9066\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.90820\n",
            "Epoch 4/40\n",
            "1562/1562 [==============================] - 501s 321ms/step - loss: 0.3341 - acc: 0.9234 - val_loss: 0.4268 - val_acc: 0.9063\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.90820\n",
            "Epoch 5/40\n",
            "1562/1562 [==============================] - 496s 317ms/step - loss: 0.3342 - acc: 0.9237 - val_loss: 0.4148 - val_acc: 0.9088\n",
            "\n",
            "Epoch 00005: val_acc improved from 0.90820 to 0.90880, saving model to /content/gdrive/My Drive/Final_version_1_weights-improvement-05-0.91.hdf5\n",
            "Epoch 6/40\n",
            "1562/1562 [==============================] - 500s 320ms/step - loss: 0.3266 - acc: 0.9251 - val_loss: 0.4176 - val_acc: 0.9086\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.90880\n",
            "Epoch 7/40\n",
            "1562/1562 [==============================] - 502s 322ms/step - loss: 0.3274 - acc: 0.9232 - val_loss: 0.4142 - val_acc: 0.9070\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.90880\n",
            "Epoch 8/40\n",
            "1562/1562 [==============================] - 505s 323ms/step - loss: 0.3233 - acc: 0.9263 - val_loss: 0.4106 - val_acc: 0.9087\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.90880\n",
            "Epoch 9/40\n",
            "1562/1562 [==============================] - 507s 324ms/step - loss: 0.3212 - acc: 0.9259 - val_loss: 0.4093 - val_acc: 0.9092\n",
            "\n",
            "Epoch 00009: val_acc improved from 0.90880 to 0.90920, saving model to /content/gdrive/My Drive/Final_version_1_weights-improvement-09-0.91.hdf5\n",
            "Epoch 10/40\n",
            "1562/1562 [==============================] - 509s 326ms/step - loss: 0.3207 - acc: 0.9260 - val_loss: 0.4183 - val_acc: 0.9078\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.90920\n",
            "Epoch 11/40\n",
            "1562/1562 [==============================] - 504s 323ms/step - loss: 0.3164 - acc: 0.9273 - val_loss: 0.4177 - val_acc: 0.9097\n",
            "\n",
            "Epoch 00011: val_acc improved from 0.90920 to 0.90970, saving model to /content/gdrive/My Drive/Final_version_1_weights-improvement-11-0.91.hdf5\n",
            "Epoch 12/40\n",
            "1562/1562 [==============================] - 506s 324ms/step - loss: 0.3152 - acc: 0.9259 - val_loss: 0.4144 - val_acc: 0.9104\n",
            "\n",
            "Epoch 00012: val_acc improved from 0.90970 to 0.91040, saving model to /content/gdrive/My Drive/Final_version_1_weights-improvement-12-0.91.hdf5\n",
            "Epoch 13/40\n",
            "1562/1562 [==============================] - 508s 325ms/step - loss: 0.3175 - acc: 0.9268 - val_loss: 0.4071 - val_acc: 0.9111\n",
            "\n",
            "Epoch 00013: val_acc improved from 0.91040 to 0.91110, saving model to /content/gdrive/My Drive/Final_version_1_weights-improvement-13-0.91.hdf5\n",
            "Epoch 14/40\n",
            "1562/1562 [==============================] - 503s 322ms/step - loss: 0.3096 - acc: 0.9289 - val_loss: 0.4156 - val_acc: 0.9098\n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.91110\n",
            "Epoch 15/40\n",
            "1562/1562 [==============================] - 508s 325ms/step - loss: 0.3094 - acc: 0.9282 - val_loss: 0.4161 - val_acc: 0.9108\n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.91110\n",
            "Epoch 16/40\n",
            "1562/1562 [==============================] - 505s 323ms/step - loss: 0.3091 - acc: 0.9292 - val_loss: 0.4246 - val_acc: 0.9071\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.91110\n",
            "Epoch 17/40\n",
            "1562/1562 [==============================] - 502s 321ms/step - loss: 0.3092 - acc: 0.9287 - val_loss: 0.4074 - val_acc: 0.9113\n",
            "\n",
            "Epoch 00017: val_acc improved from 0.91110 to 0.91130, saving model to /content/gdrive/My Drive/Final_version_1_weights-improvement-17-0.91.hdf5\n",
            "Epoch 18/40\n",
            "1562/1562 [==============================] - 505s 323ms/step - loss: 0.3079 - acc: 0.9287 - val_loss: 0.4123 - val_acc: 0.9099\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.91130\n",
            "Epoch 19/40\n",
            "1562/1562 [==============================] - 507s 325ms/step - loss: 0.3034 - acc: 0.9301 - val_loss: 0.4058 - val_acc: 0.9116\n",
            "\n",
            "Epoch 00019: val_acc improved from 0.91130 to 0.91160, saving model to /content/gdrive/My Drive/Final_version_1_weights-improvement-19-0.91.hdf5\n",
            "Epoch 20/40\n",
            "1562/1562 [==============================] - 505s 323ms/step - loss: 0.3052 - acc: 0.9297 - val_loss: 0.4149 - val_acc: 0.9091\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.91160\n",
            "Epoch 21/40\n",
            "1562/1562 [==============================] - 507s 325ms/step - loss: 0.2981 - acc: 0.9314 - val_loss: 0.4123 - val_acc: 0.9109\n",
            "\n",
            "Epoch 00021: val_acc did not improve from 0.91160\n",
            "Epoch 22/40\n",
            "1562/1562 [==============================] - 508s 326ms/step - loss: 0.2954 - acc: 0.9336 - val_loss: 0.4111 - val_acc: 0.9102\n",
            "\n",
            "Epoch 00022: val_acc did not improve from 0.91160\n",
            "Epoch 23/40\n",
            "1562/1562 [==============================] - 505s 323ms/step - loss: 0.2963 - acc: 0.9320 - val_loss: 0.4105 - val_acc: 0.9115\n",
            "\n",
            "Epoch 00023: val_acc did not improve from 0.91160\n",
            "Epoch 24/40\n",
            "1562/1562 [==============================] - 508s 325ms/step - loss: 0.2975 - acc: 0.9324 - val_loss: 0.4089 - val_acc: 0.9119\n",
            "\n",
            "Epoch 00024: val_acc improved from 0.91160 to 0.91190, saving model to /content/gdrive/My Drive/Final_version_1_weights-improvement-24-0.91.hdf5\n",
            "Epoch 25/40\n",
            "1562/1562 [==============================] - 508s 325ms/step - loss: 0.2949 - acc: 0.9325 - val_loss: 0.4077 - val_acc: 0.9123\n",
            "\n",
            "Epoch 00025: val_acc improved from 0.91190 to 0.91230, saving model to /content/gdrive/My Drive/Final_version_1_weights-improvement-25-0.91.hdf5\n",
            "Epoch 26/40\n",
            "1562/1562 [==============================] - 495s 317ms/step - loss: 0.2968 - acc: 0.9325 - val_loss: 0.4112 - val_acc: 0.9113\n",
            "\n",
            "Epoch 00026: val_acc did not improve from 0.91230\n",
            "Epoch 27/40\n",
            "1562/1562 [==============================] - 494s 316ms/step - loss: 0.2957 - acc: 0.9324 - val_loss: 0.4107 - val_acc: 0.9106\n",
            "\n",
            "Epoch 00027: val_acc did not improve from 0.91230\n",
            "Epoch 28/40\n",
            "1562/1562 [==============================] - 500s 320ms/step - loss: 0.2943 - acc: 0.9328 - val_loss: 0.4113 - val_acc: 0.9112\n",
            "\n",
            "Epoch 00028: val_acc did not improve from 0.91230\n",
            "Epoch 29/40\n",
            "1562/1562 [==============================] - 495s 317ms/step - loss: 0.2952 - acc: 0.9329 - val_loss: 0.4088 - val_acc: 0.9115\n",
            "\n",
            "Epoch 00029: val_acc did not improve from 0.91230\n",
            "Epoch 30/40\n",
            "1562/1562 [==============================] - 496s 317ms/step - loss: 0.2936 - acc: 0.9332 - val_loss: 0.4091 - val_acc: 0.9109\n",
            "\n",
            "Epoch 00030: val_acc did not improve from 0.91230\n",
            "Epoch 31/40\n",
            "1562/1562 [==============================] - 495s 317ms/step - loss: 0.2941 - acc: 0.9326 - val_loss: 0.4095 - val_acc: 0.9114\n",
            "\n",
            "Epoch 00031: val_acc did not improve from 0.91230\n",
            "Epoch 32/40\n",
            "1562/1562 [==============================] - 495s 317ms/step - loss: 0.2932 - acc: 0.9326 - val_loss: 0.4089 - val_acc: 0.9110\n",
            "\n",
            "Epoch 00032: val_acc did not improve from 0.91230\n",
            "Epoch 33/40\n",
            "1562/1562 [==============================] - 498s 319ms/step - loss: 0.2932 - acc: 0.9334 - val_loss: 0.4084 - val_acc: 0.9111\n",
            "\n",
            "Epoch 00033: val_acc did not improve from 0.91230\n",
            "Epoch 34/40\n",
            "1562/1562 [==============================] - 499s 320ms/step - loss: 0.2918 - acc: 0.9341 - val_loss: 0.4061 - val_acc: 0.9117\n",
            "\n",
            "Epoch 00034: val_acc did not improve from 0.91230\n",
            "Epoch 35/40\n",
            "1562/1562 [==============================] - 500s 320ms/step - loss: 0.2920 - acc: 0.9335 - val_loss: 0.4078 - val_acc: 0.9116\n",
            "\n",
            "Epoch 00035: val_acc did not improve from 0.91230\n",
            "Epoch 36/40\n",
            "1562/1562 [==============================] - 506s 324ms/step - loss: 0.2954 - acc: 0.9332 - val_loss: 0.4084 - val_acc: 0.9111\n",
            "\n",
            "Epoch 00036: val_acc did not improve from 0.91230\n",
            "Epoch 37/40\n",
            "1562/1562 [==============================] - 507s 324ms/step - loss: 0.2929 - acc: 0.9336 - val_loss: 0.4073 - val_acc: 0.9114\n",
            "\n",
            "Epoch 00037: val_acc did not improve from 0.91230\n",
            "Epoch 38/40\n",
            "1562/1562 [==============================] - 503s 322ms/step - loss: 0.2928 - acc: 0.9328 - val_loss: 0.4071 - val_acc: 0.9115\n",
            "\n",
            "Epoch 00038: val_acc did not improve from 0.91230\n",
            "Epoch 39/40\n",
            "1562/1562 [==============================] - 507s 325ms/step - loss: 0.2926 - acc: 0.9325 - val_loss: 0.4061 - val_acc: 0.9122\n",
            "\n",
            "Epoch 00039: val_acc did not improve from 0.91230\n",
            "Epoch 40/40\n",
            "1562/1562 [==============================] - 502s 322ms/step - loss: 0.2910 - acc: 0.9339 - val_loss: 0.4064 - val_acc: 0.9132\n",
            "\n",
            "Epoch 00040: val_acc improved from 0.91230 to 0.91320, saving model to /content/gdrive/My Drive/Final_version_1_weights-improvement-40-0.91.hdf5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fefabdd5978>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "metadata": {
        "id": "B258emM9Z1-B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def lr_schedule(epochs):\n",
        "    lrate = 0.0001\n",
        "    if epochs >20:\n",
        "      lrate = 0.00001\n",
        "    elif epochs >=35:\n",
        "      lrate=0.0001     \n",
        "    return lrate\n",
        "  \n",
        "sgd_ = SGD(lr=0.001, momentum=0.9,decay=1e-6, nesterov=True)\n",
        "  \n",
        "# determine Loss function and Optimizer\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=sgd_,\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "E_Jf4D-dZ_35",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.load_weights('/content/gdrive/My Drive/Final_version_1_weights-improvement-40-0.91.hdf5')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FdvD_Fobb-BR",
        "colab_type": "code",
        "outputId": "3187b1dc-ac5a-45b5-f30f-64fcf958112a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2791
        }
      },
      "cell_type": "code",
      "source": [
        "history1=model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
        "                    epochs=epochs, verbose=1,\n",
        "                    validation_data=(x_test,y_test),\n",
        "                 callbacks=[LearningRateScheduler(step_decay),checkpoint])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Variable *= will be deprecated. Use `var.assign(var * other)` if you want assignment to the variable value or `x = x * y` if you want a new python Tensor object.\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/40\n",
            "50000/50000 [==============================] - 275s 6ms/step - loss: 0.2515 - acc: 0.9473 - val_loss: 0.3704 - val_acc: 0.9163\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.91630, saving model to /content/gdrive/My Drive/Final_version_1_weights-improvement-01-0.92.hdf5\n",
            "Epoch 2/40\n",
            "50000/50000 [==============================] - 266s 5ms/step - loss: 0.2478 - acc: 0.9488 - val_loss: 0.3747 - val_acc: 0.9175\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.91630 to 0.91750, saving model to /content/gdrive/My Drive/Final_version_1_weights-improvement-02-0.92.hdf5\n",
            "Epoch 3/40\n",
            "50000/50000 [==============================] - 266s 5ms/step - loss: 0.2433 - acc: 0.9504 - val_loss: 0.3744 - val_acc: 0.9184\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.91750 to 0.91840, saving model to /content/gdrive/My Drive/Final_version_1_weights-improvement-03-0.92.hdf5\n",
            "Epoch 4/40\n",
            "50000/50000 [==============================] - 266s 5ms/step - loss: 0.2418 - acc: 0.9510 - val_loss: 0.3710 - val_acc: 0.9204\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.91840 to 0.92040, saving model to /content/gdrive/My Drive/Final_version_1_weights-improvement-04-0.92.hdf5\n",
            "Epoch 5/40\n",
            "50000/50000 [==============================] - 266s 5ms/step - loss: 0.2366 - acc: 0.9525 - val_loss: 0.3647 - val_acc: 0.9223\n",
            "\n",
            "Epoch 00005: val_acc improved from 0.92040 to 0.92230, saving model to /content/gdrive/My Drive/Final_version_1_weights-improvement-05-0.92.hdf5\n",
            "Epoch 6/40\n",
            "50000/50000 [==============================] - 266s 5ms/step - loss: 0.2343 - acc: 0.9531 - val_loss: 0.3804 - val_acc: 0.9178\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.92230\n",
            "Epoch 7/40\n",
            "50000/50000 [==============================] - 266s 5ms/step - loss: 0.2331 - acc: 0.9531 - val_loss: 0.3723 - val_acc: 0.9186\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.92230\n",
            "Epoch 8/40\n",
            "50000/50000 [==============================] - 266s 5ms/step - loss: 0.2291 - acc: 0.9545 - val_loss: 0.3815 - val_acc: 0.9158\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.92230\n",
            "Epoch 9/40\n",
            "50000/50000 [==============================] - 266s 5ms/step - loss: 0.2284 - acc: 0.9563 - val_loss: 0.3813 - val_acc: 0.9179\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.92230\n",
            "Epoch 10/40\n",
            "50000/50000 [==============================] - 266s 5ms/step - loss: 0.2239 - acc: 0.9563 - val_loss: 0.3711 - val_acc: 0.9205\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.92230\n",
            "Epoch 11/40\n",
            "50000/50000 [==============================] - 266s 5ms/step - loss: 0.2230 - acc: 0.9566 - val_loss: 0.3788 - val_acc: 0.9177\n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.92230\n",
            "Epoch 12/40\n",
            "50000/50000 [==============================] - 266s 5ms/step - loss: 0.2229 - acc: 0.9565 - val_loss: 0.3791 - val_acc: 0.9175\n",
            "\n",
            "Epoch 00012: val_acc did not improve from 0.92230\n",
            "Epoch 13/40\n",
            "50000/50000 [==============================] - 266s 5ms/step - loss: 0.2196 - acc: 0.9574 - val_loss: 0.3751 - val_acc: 0.9195\n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.92230\n",
            "Epoch 14/40\n",
            "50000/50000 [==============================] - 266s 5ms/step - loss: 0.2184 - acc: 0.9579 - val_loss: 0.3738 - val_acc: 0.9201\n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.92230\n",
            "Epoch 15/40\n",
            "50000/50000 [==============================] - 266s 5ms/step - loss: 0.2176 - acc: 0.9589 - val_loss: 0.3775 - val_acc: 0.9191\n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.92230\n",
            "Epoch 16/40\n",
            "50000/50000 [==============================] - 266s 5ms/step - loss: 0.2196 - acc: 0.9572 - val_loss: 0.3813 - val_acc: 0.9191\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.92230\n",
            "Epoch 17/40\n",
            "50000/50000 [==============================] - 266s 5ms/step - loss: 0.2154 - acc: 0.9586 - val_loss: 0.3783 - val_acc: 0.9199\n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.92230\n",
            "Epoch 18/40\n",
            "50000/50000 [==============================] - 266s 5ms/step - loss: 0.2129 - acc: 0.9593 - val_loss: 0.3752 - val_acc: 0.9197\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.92230\n",
            "Epoch 19/40\n",
            "50000/50000 [==============================] - 266s 5ms/step - loss: 0.2136 - acc: 0.9595 - val_loss: 0.3816 - val_acc: 0.9177\n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.92230\n",
            "Epoch 20/40\n",
            "50000/50000 [==============================] - 266s 5ms/step - loss: 0.2175 - acc: 0.9582 - val_loss: 0.3803 - val_acc: 0.9181\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.92230\n",
            "Epoch 21/40\n",
            "50000/50000 [==============================] - 263s 5ms/step - loss: 0.2123 - acc: 0.9591 - val_loss: 0.3777 - val_acc: 0.9188\n",
            "\n",
            "Epoch 00021: val_acc did not improve from 0.92230\n",
            "Epoch 22/40\n",
            "50000/50000 [==============================] - 263s 5ms/step - loss: 0.2128 - acc: 0.9591 - val_loss: 0.3826 - val_acc: 0.9179\n",
            "\n",
            "Epoch 00022: val_acc did not improve from 0.92230\n",
            "Epoch 23/40\n",
            "50000/50000 [==============================] - 263s 5ms/step - loss: 0.2106 - acc: 0.9601 - val_loss: 0.3830 - val_acc: 0.9189\n",
            "\n",
            "Epoch 00023: val_acc did not improve from 0.92230\n",
            "Epoch 24/40\n",
            "50000/50000 [==============================] - 263s 5ms/step - loss: 0.2107 - acc: 0.9597 - val_loss: 0.3823 - val_acc: 0.9189\n",
            "\n",
            "Epoch 00024: val_acc did not improve from 0.92230\n",
            "Epoch 25/40\n",
            "50000/50000 [==============================] - 263s 5ms/step - loss: 0.2086 - acc: 0.9607 - val_loss: 0.3825 - val_acc: 0.9187\n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.92230\n",
            "Epoch 26/40\n",
            "50000/50000 [==============================] - 262s 5ms/step - loss: 0.2079 - acc: 0.9609 - val_loss: 0.3799 - val_acc: 0.9195\n",
            "\n",
            "Epoch 00026: val_acc did not improve from 0.92230\n",
            "Epoch 27/40\n",
            "50000/50000 [==============================] - 263s 5ms/step - loss: 0.2072 - acc: 0.9616 - val_loss: 0.3823 - val_acc: 0.9195\n",
            "\n",
            "Epoch 00027: val_acc did not improve from 0.92230\n",
            "Epoch 28/40\n",
            "50000/50000 [==============================] - 263s 5ms/step - loss: 0.2086 - acc: 0.9606 - val_loss: 0.3814 - val_acc: 0.9191\n",
            "\n",
            "Epoch 00028: val_acc did not improve from 0.92230\n",
            "Epoch 29/40\n",
            "50000/50000 [==============================] - 262s 5ms/step - loss: 0.2067 - acc: 0.9611 - val_loss: 0.3798 - val_acc: 0.9201\n",
            "\n",
            "Epoch 00029: val_acc did not improve from 0.92230\n",
            "Epoch 30/40\n",
            "50000/50000 [==============================] - 262s 5ms/step - loss: 0.2086 - acc: 0.9601 - val_loss: 0.3823 - val_acc: 0.9199\n",
            "\n",
            "Epoch 00030: val_acc did not improve from 0.92230\n",
            "Epoch 31/40\n",
            "50000/50000 [==============================] - 262s 5ms/step - loss: 0.2074 - acc: 0.9608 - val_loss: 0.3824 - val_acc: 0.9192\n",
            "\n",
            "Epoch 00031: val_acc did not improve from 0.92230\n",
            "Epoch 32/40\n",
            "50000/50000 [==============================] - 262s 5ms/step - loss: 0.2057 - acc: 0.9627 - val_loss: 0.3824 - val_acc: 0.9196\n",
            "\n",
            "Epoch 00032: val_acc did not improve from 0.92230\n",
            "Epoch 33/40\n",
            "50000/50000 [==============================] - 262s 5ms/step - loss: 0.2062 - acc: 0.9612 - val_loss: 0.3830 - val_acc: 0.9189\n",
            "\n",
            "Epoch 00033: val_acc did not improve from 0.92230\n",
            "Epoch 34/40\n",
            "50000/50000 [==============================] - 263s 5ms/step - loss: 0.2079 - acc: 0.9601 - val_loss: 0.3829 - val_acc: 0.9193\n",
            "\n",
            "Epoch 00034: val_acc did not improve from 0.92230\n",
            "Epoch 35/40\n",
            "50000/50000 [==============================] - 262s 5ms/step - loss: 0.2059 - acc: 0.9615 - val_loss: 0.3838 - val_acc: 0.9193\n",
            "\n",
            "Epoch 00035: val_acc did not improve from 0.92230\n",
            "Epoch 36/40\n",
            "50000/50000 [==============================] - 262s 5ms/step - loss: 0.2063 - acc: 0.9612 - val_loss: 0.3839 - val_acc: 0.9189\n",
            "\n",
            "Epoch 00036: val_acc did not improve from 0.92230\n",
            "Epoch 37/40\n",
            "50000/50000 [==============================] - 262s 5ms/step - loss: 0.2045 - acc: 0.9624 - val_loss: 0.3849 - val_acc: 0.9189\n",
            "\n",
            "Epoch 00037: val_acc did not improve from 0.92230\n",
            "Epoch 38/40\n",
            "50000/50000 [==============================] - 263s 5ms/step - loss: 0.2054 - acc: 0.9623 - val_loss: 0.3838 - val_acc: 0.9198\n",
            "\n",
            "Epoch 00038: val_acc did not improve from 0.92230\n",
            "Epoch 39/40\n",
            "50000/50000 [==============================] - 263s 5ms/step - loss: 0.2037 - acc: 0.9625 - val_loss: 0.3861 - val_acc: 0.9189\n",
            "\n",
            "Epoch 00039: val_acc did not improve from 0.92230\n",
            "Epoch 40/40\n",
            "50000/50000 [==============================] - 263s 5ms/step - loss: 0.2020 - acc: 0.9622 - val_loss: 0.3836 - val_acc: 0.9200\n",
            "\n",
            "Epoch 00040: val_acc did not improve from 0.92230\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bXJh69sw87z-",
        "colab_type": "code",
        "outputId": "6530f6ae-3ae6-40b2-b8a4-22962af6922b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "model.load_weights('/content/gdrive/My Drive/Final_version_1_weights-improvement-05-0.92.hdf5')\n",
        "\n",
        "# Test the model\n",
        "score = model.evaluate(x_test, y_test, verbose=1)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 22s 2ms/step\n",
            "Test loss: 0.36465714502334595\n",
            "Test accuracy: 0.9223\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "r1nI4408iQXI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# history2=model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
        "#                     steps_per_epoch=(x_train.shape[0] // (batch_size/2)), epochs=epochs, verbose=1,\n",
        "#                     validation_data=(x_test,y_test),\n",
        "#                  callbacks=[lrate,checkpoint])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ilRstyBuTOXt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# import math\n",
        "\n",
        "# def step_decay(epoch):\n",
        "#   initial_lrate = 0.001\n",
        "#   drop = 0.5\n",
        "#   epochs_drop = 5.0\n",
        "#   lrate = initial_lrate * math.pow(drop,  \n",
        "#            math.floor((1+epoch)/epochs_drop))\n",
        "#   return lrate\n",
        "# lrate = LearningRateScheduler(step_decay)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "I6M2LMd99Fr0",
        "colab_type": "code",
        "outputId": "586f031c-a503-433c-dffb-847471c07c98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "model.save_weights('model_weights.h5')\n",
        "weights_file = drive.CreateFile({'Final_version_1' : 'model_weights.h5'})\n",
        "weights_file.SetContentFile('model_weights.h5')\n",
        "weights_file.Upload()\n",
        "drive.CreateFile({'id': weights_file.get('id')})"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GoogleDriveFile({'id': '1Xv--moa1eMaaqWqZhxedQ-H1Lqb85Ako'})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "metadata": {
        "id": "iMA1FJjJoz-S",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-l1YckApfqOW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}